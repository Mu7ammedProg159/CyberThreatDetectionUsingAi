{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41d6772b-3e33-49c9-b078-19750de1848a",
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, learning_curve, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix, auc\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.utils import resample\n",
    "from sklearn.svm import OneClassSVM\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from functions import preprocessing_data\n",
    "import torch\n",
    "import xgboost as xgb\n",
    "from joblib import dump\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f407ea88-1b5f-4cbc-b5e1-5cbcc88fc0d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load The Split Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060c4e1b",
   "metadata": {},
   "source": [
    "Please First run ThreatDetection.ipynb till Cell 17 (Before XGBoost Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd444347",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('SplitDatasets/features_train.csv')\n",
    "y_train = pd.read_csv('SplitDatasets/labels_train.csv')\n",
    "\n",
    "X_val = pd.read_csv('SplitDatasets/features_val.csv')\n",
    "y_val = pd.read_csv('SplitDatasets/labels_val.csv')\n",
    "\n",
    "X_test = pd.read_csv('SplitDatasets/features_test.csv')\n",
    "y_test = pd.read_csv('SplitDatasets/labels_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88740df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train['label'].values\n",
    "y_test = y_test['label'].values\n",
    "y_val = y_val['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4401984",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Define the XGBoost classifier\n",
    "xgb_classifier = xgb.XGBClassifier(objective='multi:softmax', learning_rate= 0.01, max_depth = 3,\n",
    "                                   reg_alpha = 0.01, reg_lambda = 0.01, num_class=3, seed=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Save the XGBoost Classifier Model in dumb file to avoid retraining process. (It is used in ThreatDetection.ipynb).\n",
    "dump(xgb_classifier, 'TrainingDumpData/MachineLearning/XGBoost_Model.joblib')\n",
    "print('Training saved successfully!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cc3dfb-f0db-46c6-a83c-513d65973aa0",
   "metadata": {},
   "source": [
    "# Logistic Regression implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b65fdd4-1640-448b-b5ae-bab58ba68eee",
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TrainingDumpData/MachineLearning/LogReg_Model.joblib']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Logistic Regression classifier with commonly used hyperparameters\n",
    "logreg_classifier = LogisticRegression()\n",
    "\n",
    "# Train the classifier\n",
    "logreg_classifier.fit(X_train, y_train)\n",
    "\n",
    "#Save Logistic Regression Classificaion Model in Dumb File using JobLib library.\n",
    "dump(logreg_classifier, 'TrainingDumpData/MachineLearning/LogReg_Model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608d236c-876b-4967-9e4d-42796a48da20",
   "metadata": {
    "tags": []
   },
   "source": [
    "# K-Nearest Classifier implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e46c7f81-c7ca-439c-94ce-41298888e1df",
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TrainingDumpData/MachineLearning/Knn_Model.joblib']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create KNN classifier with commonly used hyperparameters\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "\n",
    "# Train the classifier\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "#Save Logistic Regression Classificaion Model in Dumb File using JobLib library.\n",
    "dump(knn_classifier, 'TrainingDumpData/MachineLearning/Knn_Model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b1766c-d975-4a02-b049-b9009dfec1be",
   "metadata": {},
   "source": [
    "# Random Forest Classifier implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a02a7cea-787f-4292-ae81-e17b9f6c0cbe",
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TrainingDumpData/MachineLearning/RF_Model.joblib']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create Random Forest classifier with commonly used hyperparameters\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# Train the classifier\n",
    "rf_classifier = rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "#Save Random Forest classifier Model in Dumb File using JobLib library.\n",
    "dump(rf_classifier, 'TrainingDumpData/MachineLearning/RF_Model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0708ceed-f846-4faf-900e-4b7a99163e4b",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes Classifier implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d755376-09ed-4dd1-af40-16182f79d976",
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TrainingDumpData/MachineLearning/GNBayes_Model.joblib']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Train the classifier\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "#Save Gaussian Naive Bayes classifier Model in Dumb File using JobLib library.\n",
    "dump(gnb, 'TrainingDumpData/MachineLearning/GNBayes_Model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31041658-78e6-4447-bb1b-fc59c77c86f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Decision Tree Classifier implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a4eec9e-246e-453b-af0d-9fd72b370f50",
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TrainingDumpData/MachineLearning/DT_Model.joblib']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training random forest clf\n",
    "dt_classifier = tree.DecisionTreeClassifier(criterion='gini', splitter=\"best\", max_leaf_nodes=7,\n",
    "                                            min_samples_leaf=1, min_samples_split=2, max_depth=None, random_state=42)\n",
    "dt_classifier = dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "#Save Decision Tree classifier Model in Dumb File using JobLib library.\n",
    "dump(dt_classifier, 'TrainingDumpData/MachineLearning/DT_Model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f88c3e9",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba699687",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ninja\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18914/18914\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 3ms/step - accuracy: 0.9272 - loss: 0.2645 - val_accuracy: 0.9874 - val_loss: 0.0565\n",
      "Epoch 2/10\n",
      "\u001b[1m18914/18914\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 3ms/step - accuracy: 0.9873 - loss: 0.0533 - val_accuracy: 0.9909 - val_loss: 0.0490\n",
      "Epoch 3/10\n",
      "\u001b[1m18914/18914\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 3ms/step - accuracy: 0.9891 - loss: 0.0460 - val_accuracy: 0.9910 - val_loss: 0.0450\n",
      "Epoch 4/10\n",
      "\u001b[1m18914/18914\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 3ms/step - accuracy: 0.9897 - loss: 0.0434 - val_accuracy: 0.9914 - val_loss: 0.0485\n",
      "Epoch 5/10\n",
      "\u001b[1m18914/18914\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 3ms/step - accuracy: 0.9898 - loss: 0.0418 - val_accuracy: 0.9907 - val_loss: 0.0387\n",
      "Epoch 6/10\n",
      "\u001b[1m18914/18914\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 3ms/step - accuracy: 0.9900 - loss: 0.0399 - val_accuracy: 0.9882 - val_loss: 0.0442\n",
      "Epoch 7/10\n",
      "\u001b[1m18914/18914\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 3ms/step - accuracy: 0.9904 - loss: 0.0391 - val_accuracy: 0.9907 - val_loss: 0.0361\n",
      "Epoch 8/10\n",
      "\u001b[1m18914/18914\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 3ms/step - accuracy: 0.9904 - loss: 0.0385 - val_accuracy: 0.9919 - val_loss: 0.0418\n",
      "Epoch 9/10\n",
      "\u001b[1m18914/18914\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 3ms/step - accuracy: 0.9905 - loss: 0.0380 - val_accuracy: 0.9919 - val_loss: 0.0404\n",
      "Epoch 10/10\n",
      "\u001b[1m18914/18914\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 3ms/step - accuracy: 0.9907 - loss: 0.0375 - val_accuracy: 0.9905 - val_loss: 0.0360\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "from tensorflow.keras.layers import GRU, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "import json\n",
    "from keras.models import load_model\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test, X_val, y_val are your data\n",
    "# Convert DataFrame objects to numpy arrays\n",
    "X_train_array = X_train.values\n",
    "X_test_array = X_test.values\n",
    "X_val_array = X_val.values\n",
    "\n",
    "# Reshape the input data to match the GRU input shape\n",
    "X_train_reshaped = np.reshape(X_train_array, (X_train_array.shape[0], X_train_array.shape[1], 1))\n",
    "X_val_reshaped = np.reshape(X_val_array, (X_val_array.shape[0], X_val_array.shape[1], 1))\n",
    "X_test_reshaped = X_test_array.reshape(X_test_array.shape[0], X_test_array.shape[1], 1)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    GRU(units=64, input_shape=(X_train_reshaped.shape[1], 1)),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model with a specific learning rate\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, validation_data=(X_val_reshaped, y_val))\n",
    "\n",
    "# Save the training history to a JSON file\n",
    "with open('TrainingDumpData/DeepLearning/history.json', 'w') as json_file:\n",
    "    json.dump(history.history, json_file)\n",
    "\n",
    "# Save model architecture as JSON\n",
    "model_json = model.to_json()\n",
    "with open('TrainingDumpData/DeepLearning/gru_model_architecture.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "# Save model weights\n",
    "model.save_weights(\"TrainingDumpData/DeepLearning/gru_model.weights.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "# ---------------------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d7a2dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('SplitDatasets/Reshaped/X_train_reshaped.npy', X_train_reshaped)\n",
    "np.save('SplitDatasets/Reshaped/X_val_reshaped.npy', X_val_reshaped)\n",
    "np.save('SplitDatasets/Reshaped/X_test_reshaped.npy', X_test_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67133179-c8fa-44de-8afa-a02695bfc711",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
